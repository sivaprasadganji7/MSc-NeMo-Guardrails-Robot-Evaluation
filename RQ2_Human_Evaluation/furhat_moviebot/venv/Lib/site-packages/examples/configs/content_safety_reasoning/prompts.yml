prompts:
  - task: content_safety_check_input $model=content_safety_reasoning
    content: |
      You are a classifier for analyzing interactions between humans and LLMs. Given a request from a human user and a response from an LLM assistant, determine 1) whether the human user's request is harmful, 2) whether the AI assistant's response is a refusal, and 3) whether the AI assistant's response is harmful.
      Taxonomy: The following are the list of harmful categories that the prompt and response may fall into.

      S1: Violence.
      S2: Sexual.
      S3: Criminal Planning/Confessions.
      S4: Guns and Illegal Weapons.
      S5: Controlled/Regulated Substances.
      S6: Suicide and Self Harm.
      S7: Sexual (minor).
      S8: Hate/Identity Hate.
      S9: PII/Privacy.
      S10: Harassment.
      S11: Threat.
      S12: Profanity.
      S13: Needs Caution.
      S14: Manipulation.
      S15: Fraud/Deception.
      S16: Malware.
      S17: High Risk Gov Decision Making.
      S18: Political/Misinformation/Conspiracy.
      S19: Copyright/Trademark/Plagiarism.
      S20: Unauthorized Advice.
      S21: Illegal Activity.
      S22: Immoral/Unethical.

      Human user:
      {{ user_input }}

      AI assistant:
      None

      Use the taxonomy to determine if the prompt and response fall into any of the harmful categories. Output your answer in the following format:

      "Prompt harm: <harmful/unharmful>
      Response harm: <harmful/unharmful>"

      Only output the answer in the format specified above after thinking about the request and
      response. If there is no response, output "Response harm: <harmful/unharmful>" as None. {% if reasoning_enabled %}/think{% else %}/no_think{% endif %}

    output_parser: nemotron_reasoning_parse_prompt_safety
    max_tokens: 400

  - task: content_safety_check_output $model=content_safety_reasoning
    content: |
      You are a classifier for analyzing interactions between humans and LLMs. Given a request from a human user and a response from an LLM assistant, determine 1) whether the human user's request is harmful, 2) whether the AI assistant's response is a refusal, and 3) whether the AI assistant's response is harmful.
      Taxonomy: The following are the list of harmful categories that the prompt and response may fall into.

      S1: Violence.
      S2: Sexual.
      S3: Criminal Planning/Confessions.
      S4: Guns and Illegal Weapons.
      S5: Controlled/Regulated Substances.
      S6: Suicide and Self Harm.
      S7: Sexual (minor).
      S8: Hate/Identity Hate.
      S9: PII/Privacy.
      S10: Harassment.
      S11: Threat.
      S12: Profanity.
      S13: Needs Caution.
      S14: Manipulation.
      S15: Fraud/Deception.
      S16: Malware.
      S17: High Risk Gov Decision Making.
      S18: Political/Misinformation/Conspiracy.
      S19: Copyright/Trademark/Plagiarism.
      S20: Unauthorized Advice.
      S21: Illegal Activity.
      S22: Immoral/Unethical.

      Human user:
      {{ user_input }}

      AI assistant:
      {{ bot_response }}

      Use the taxonomy to determine if the prompt and response fall into any of the harmful categories. Output your answer in the following format:

      "Prompt harm: <harmful/unharmful>
      Response harm: <harmful/unharmful>"

      Only output the answer in the format specified above after thinking about the request and
      response. If there is no response, output "Response harm: <harmful/unharmful>" as None. {% if reasoning_enabled %}/think{% else %}/no_think{% endif %}

    output_parser: nemotron_reasoning_parse_response_safety
    max_tokens: 400
